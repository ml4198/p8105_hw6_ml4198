---
title: "p8105_hw6_ml4198"
author: "Matthew Lawlor"
date: "12/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(modelr)
```

## Problem 1

### Read and tidy

```{r}
homicide_df = read.csv("./data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  filter(city != "Dallas", city != "Phoenix", city != "Kansas City", city != "Tulsa", victim_race %in% c("White", "Black")) %>% 
  mutate(
    city_state = paste(city, state, sep = ", "),
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")
  ) %>% 
  select(resolved, city_state, victim_age, victim_sex, victim_race)
```

### Fit logistic regression model

```{r}
fit_baltimore =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())
```

Now tidy

```{r}
fit_baltimore %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>% 
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```

Adjusted OR for case resolved in black v white victims is 0.431.

### Regress all cities

```{r}
fit_all =
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    model = map(.x = data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(model, broom::tidy)
  ) %>% 
  unnest(results) %>%
  filter(term == "victim_raceBlack") %>% 
  mutate(
    OR = exp(estimate),
    ci_lower = exp(estimate - std.error),
    ci_upper = exp(estimate + std.error)
  ) %>%  
  select(city_state, OR, ci_lower, ci_upper)
```

then plot

```{r}
fit_all %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = OR, y = city_state)) +
  geom_vline(aes(xintercept = 1)) +
  geom_errorbarh(aes(xmax = ci_upper, xmin = ci_lower), size = .5, height = .2, color = "gray50") +
  geom_point(color = "orange")
```

This is a plot of OR +/- CI for resolved cases for black vs white victims. There is variation across cities for odds of resolved case regressing by race. Unfortunately, many cities lie well below OR 1 suggesting homicides with black victims are less likely to be solved.

## Problem 2

Read and tidy

```{r}
birthweight_df = read.csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  drop_na() %>% 
  mutate(
    babysex = as_factor(babysex),
    frace = as_factor(frace),
    malform = as_factor(malform),
    mrace = as_factor(mrace),
  )
```

Heatmap of numeric variables

```{r}
birthweight_df %>% 
  select(-babysex, -frace, -malform, -mrace) %>% 
  cor() %>% 
  round(2) %>% 
  reshape2::melt() %>% 
  ggplot(aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Stepwise linear regression

```{r}
fit_lm = lm(bwt ~ ., data = birthweight_df)
fit_step = MASS::stepAIC(fit_lm, direction = "both", trace = FALSE) %>% 
  broom::tidy()
```

Linear regression model incorporating select predictors based on review of above

```{r}
fit_lm = lm(bwt ~ bhead + blength + babysex + mheight + delwt + ppwt + gaweeks + mrace + smoken, data=birthweight_df) 

fit_lm %>% 
  broom::tidy()
```

Plot residuals. It appears the model over-predicts in a medium birthweight range (2000-3000g) and under-predicts in a high birth weight range (3000-4000g). It additionally under predicts below 2000g but the number of cases here is limited.

```{r}
birthweight_df %>% 
  add_residuals(fit_lm) %>% 
  ggplot(aes(x = bwt, y = resid)) +
  geom_point()
```

Now compare fit to additional linear models

```{r}
cv_df = 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    fit_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    main_effects_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    interactions_mod = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))
    ) %>% 
  mutate(
    rmse_fit = map2_dbl(fit_mod, test, ~rmse(model = .x, data =.y)),
    rmse_main_effects = map2_dbl(main_effects_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interactions = map2_dbl(interactions_mod, test, ~rmse(model = .x, data= .y))
  )
```

Visualize distribution of RMSEs

```{r}
cv_df %>%
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  group_by(model) %>% 
  summarize(
    mean_rmse = mean(rmse),
    sd_rmse = sd(rmse)
  ) %>% 
  knitr::kable(digits=2)
```

After visual inspection and stepwise linear regression prior to selection of linear regression model, there seems to be no improvement over the main effects model in terms of RMSE. The interactions model has a lower mean RMSE and a narrower distribution of RMSE, suggesting this model is superior in predictive capcity to the other two.

## Problem 3

Import data

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Inspect linear regression model

```{r}

weather_fit = lm(tmax ~ tmin, data = weather_df)

weather_fit %>% broom::glance()
weather_fit %>% broom::tidy()
```

Bootstrap with `modelr`

First r squared

```{r}
bs_rsquared = 
  weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    glance_results = map(models, broom::glance)
  ) %>% 
  select(-strap, -models) %>% 
  unnest(glance_results) %>% 
  select(.id, r.squared)
  
bs_rsquared %>% 
  ggplot(aes(x = r.squared)) +
  geom_density()

bs_rsquared %>% 
  summarize(
    mean_rsquared = mean(r.squared),
    ci_lower = quantile(r.squared, 0.025),
    ci_upper = quantile(r.squared, 0.975)
  ) %>% 
  knitr::kable()
```

There is a normal distribution of R squared around mean 0.91 in the bootstrap model.

Then product intercept and slope

```{r}
bs_betas = 
  weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    tidy_results = map(models, broom::tidy)
  ) %>% 
  select(-strap, -models) %>% 
  unnest(tidy_results) %>% 
  select(-std.error, -statistic, -p.value) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  mutate(
    log_slopes = log(`(Intercept)` * tmin)
  )

bs_betas %>% 
  ggplot(aes(x = log_slopes)) +
  geom_density()

bs_betas %>% 
  summarize(
    mean_log_slopes = mean(log_slopes),
    ci_lower = quantile(log_slopes, 0.025),
    ci_upper = quantile(log_slopes, 0.975)
  ) %>% 
  knitr::kable()
```

There is a normal distribution of log(β̂ 0∗β̂ 1) around mean 2.01 in the bootstrap model.